{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 환경설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q -U bitsandbytes\n",
    "# !pip install -q -U git+https://github.com/huggingface/transformers.git \n",
    "# !pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "# !pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "# !pip install -q datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참조\n",
    "\n",
    "https://github.com/Beomi/KoAlpaca\n",
    "\n",
    "https://colab.research.google.com/gist/Beomi/f163a6c04a869d18ee1a025b6d33e6d8/2023_05_26_bnb_4bit_koalpaca_v1_1a_on_polyglot_ko_12_8b.ipynb\n",
    "\n",
    "https://wikidocs.net/238524\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CUDA 사용 가능:\", torch.cuda.is_available())\n",
    "print(\"GPU 이름:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "print(\"Torch CUDA 지원 여부:\", torch.cuda.is_available())\n",
    "print(\"CUDA 버전:\", torch.version.cuda)\n",
    "print(\"PyTorch 버전:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델, 토크나이저 로드 및 LoRA 설정, DataLoad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=False)\n",
    "\n",
    "tokenizer.pad_token = \"<|finetune_right_pad_id|>\"\n",
    "tokenizer.pad_token_id = 128004\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "샘플이 너무 적으면 과적합(Overfitting) 위험이 크므로 데이터를 증강(Augmentation)하는 것이 필요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# LLaMA 3 Chat 템플릿 적용\n",
    "chat_template = \"\"\"<|begin_of_text|><|start_header_id|>지시사항<|end_header_id|>\n",
    "{SYSTEM}<|eot_id|><|start_header_id|>입력<|end_header_id|>\n",
    "{INPUT}<|eot_id|><|start_header_id|>응답<|end_header_id|>\n",
    "{OUTPUT}<|eot_id|>\"\"\"\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "    \n",
    "    for instruction, output in zip(instructions, outputs):\n",
    "        text = chat_template.format(\n",
    "            SYSTEM=\"아래는 작업을 설명하는 지시사항입니다. 입력된 내용을 바탕으로 적절한 응답을 작성하세요.\",\n",
    "            INPUT= instruction,\n",
    "            OUTPUT= output \n",
    "        ) \n",
    "        \n",
    "        texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "# 데이터셋 로드\n",
    "dataset = load_dataset(\"json\", data_files=\"../00_Data/KoAlpaca_train.json\")\n",
    "# 데이터셋 변환\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True, remove_columns=['instruction', 'output'])\n",
    "\n",
    "split_data = dataset['train'].train_test_split(test_size=0.05, seed=42)\n",
    "train_data, val_data = split_data['train'], split_data['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_data.to_pandas()  # \"train\" 데이터셋을 pandas로 변환\n",
    "print(df.head())  # 첫 5개 샘플 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()#훈련 시 메모리 절약 (출력값을 필요할 때만 계산)\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LoRA(PEFT) 설정을 적용하여 기존 모델을 효율적으로 미세 조정\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",# LoRA를 적용할 작업 유형 (CAUSAL_LM: 언어 모델)\n",
    "    r=8,# LoRA 랭크 (적은 수록 가벼움, 크면 성능 향상 가능)\n",
    "    lora_alpha=16,   # 일반적으로 LoRA의 효과를 조절하는 파라미터 (값이 크면 LoRA 가중치의 영향 증가)\n",
    "    lora_dropout=0.05, # Dropout 확률 (일반적으로 0~0.1 추천)\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from trl import SFTTrainer\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    per_device_train_batch_size=2, # 배치 크기 (GPU당 샘플 개수)\n",
    "    gradient_accumulation_steps=1,  # 메모리 최적화 Gradient Accumulation 누적 스텝 (메모리 부족 시 증가 가능)\n",
    "    gradient_checkpointing=True, # 활성화하면 GPU 메모리 사용 감소 가능\n",
    "    num_train_epochs=1, # 전체 데이터셋을 몇 번 반복해서 학습할 것인지\n",
    "    # warmup_steps=30,  # 학습률을 서서히 증가시키는 단계 (0~100)\n",
    "    # max_steps=300 ,  # 최대 학습 스텝\n",
    "    learning_rate=2e-4,  # 학습률 (기본 2e-4)\n",
    "    lr_scheduler_type=\"linear\", # 학습률 스케줄러 종류 ( linear, cosine, constant )\n",
    "    weight_decay=0.01,\n",
    "    bf16=True, # 정밀도 ( FP32, FP16, BF16 )\n",
    "    warmup_ratio=0.1, # 전체 Step 수의 10% 동안 학습률을 점진적으로 증가\n",
    "    optim=\"adamw_torch\", # paged_adamw_8bit (VRAM절약 성능 하락) adamw_torch(정확도 높음 메모리사용량 높음)\n",
    "    seed=42,\n",
    "    output_dir=\"../01_Models/01_RoLaModels\",\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"epoch\", # 에포크 단위로 저장\n",
    "    report_to=\"none\", # WandB, TensorBoard 등 로그 저장 안 함 (필요시 변경)\n",
    "    # log_level=\"debug\",\n",
    ")\n",
    "\n",
    "# Trainer Setup\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    peft_config=lora_config,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=train_args,\n",
    "\n",
    "    # dataset_text_field=\"text\",\n",
    "    # max_seq_length=1024,\n",
    "    # packing=False,\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "# 학습 시작\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # 모델의 가중치는 변경하지 않고, forward 연산만 수행함.\n",
    "model.config.use_cache = True  # 이전 계산 결과를 저장하고 사용\t추론 속도 빨라짐, 메모리 사용 증가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 채팅 스타일 프롬프트 (Llama-3의 Chat 모델용)\n",
    "chat_prompt = \"\"\"<|begin_of_text|><|start_header_id|>지시사항<|end_header_id|>\n",
    "아래는 작업을 설명하는 지시사항입니다. 입력된 내용을 바탕으로 적절한 응답을 작성하세요.<|eot_id|>\n",
    "<|start_header_id|>입력<|end_header_id|>\n",
    "다시 합창 합시다' 처럼 거꾸로 읽어도 같은 문장이 영어에도 있나요? 또한 다른 나라의 언어에도 있는 건가요?<|eot_id|>\n",
    "<|start_header_id|>응답<|end_header_id|>\n",
    "\"\"\"\n",
    "# 토큰화 및 모델 실행\n",
    "input_ids = tokenizer(chat_prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(input_ids, max_new_tokens=100, temperature=0.7, top_p=0.9, do_sample=True)\n",
    "\n",
    "# 출력 변환\n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(\"LLM 응답:\", output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  기존 모델과 로라 병합 (어뎁터 유지) 풀파인튜닝 저장 \n",
    "\n",
    "merge_and_unload 양자화 상태로 해버리면 로라 어뎁터가 망가져버림 그래서 양자화 하지 않고 저장해줘야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "#로라 모델경로를 확인해봐야 해요\n",
    "peft_model_id = \"../01_Models/01_RoLaModels/checkpoint-19\" \n",
    "\n",
    "loadModel = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,  # float16로 유지 bfloat16\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "loadModel = PeftModel.from_pretrained(loadModel, peft_model_id, device_map=\"auto\")\n",
    "loadtokenizer = AutoTokenizer.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadModel = loadModel.merge_and_unload() #실제 병합\n",
    "merged_model_path = \"../01_Models/02_FullFinetuningModels\"\n",
    "loadModel.save_pretrained(merged_model_path)\n",
    "loadtokenizer.save_pretrained(merged_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GGUF  llama cpp 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참조\n",
    "\n",
    "https://m.blog.naver.com/112fkdldjs/223513042256\n",
    "\n",
    "https://github.com/ollama/ollama/issues/4442\n",
    "\n",
    "https://github.com/ollama/ollama/issues/4572\n",
    "\n",
    "https://github.com/teddylee777/langserve_ollama/tree/main/ollama-modelfile/Llama-3-8B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8_0: 8비트 양자화 모델로, 원본 모델의 품질을 거의 그대로 유지하면서 크기를 절반으로 줄입니다.\n",
    "\n",
    "Q6_K, Q5_K_M, Q5_K_S: 6비트와 5비트 양자화 모델들로, 품질 손실은 미미하지만 크기가 더 작습니다.\n",
    "\n",
    "Q4_K_M, Q4_K_S: 4비트 양자화 모델로, 약간의 품질 손실이 있지만 크기가 매우 작습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../llama.cpp/convert_hf_to_gguf.py ../01_Models/02_FullFinetuningModels --outtype q8_0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
