{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 환경설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q -U bitsandbytes\n",
    "# !pip install -q -U git+https://github.com/huggingface/transformers.git \n",
    "# !pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "# !pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "# !pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참조\n",
    "\n",
    "https://github.com/Beomi/KoAlpaca\n",
    "\n",
    "https://colab.research.google.com/gist/Beomi/f163a6c04a869d18ee1a025b6d33e6d8/2023_05_26_bnb_4bit_koalpaca_v1_1a_on_polyglot_ko_12_8b.ipynb\n",
    "\n",
    "https://wikidocs.net/238524\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DKSYSTEMS\\miniconda3\\envs\\dseek\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to find CUDA.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CUDA 사용 가능:\", torch.cuda.is_available())\n",
    "print(\"GPU 이름:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "print(\"Torch CUDA 지원 여부:\", torch.cuda.is_available())\n",
    "print(\"CUDA 버전:\", torch.version.cuda)\n",
    "print(\"PyTorch 버전:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델, 토크나이저 로드 및 LoRA 설정, DataLoad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "해당 모델은 meta에 키등록을 해야 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     device_map=\"auto\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "# )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=False)\n",
    "\n",
    "# tokenizer.pad_token = \"<|finetune_right_pad_id|>\"\n",
    "# tokenizer.pad_token_id = 128004\n",
    "# tokenizer.padding_side = 'right'\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # 4-bit 양자화 활성화\n",
    "    bnb_4bit_use_double_quant=True, # 이중 양자화(Double Quantization) 적용하여 더 적은 메모리 사용\n",
    "    bnb_4bit_quant_type=\"nf4\",    # 4-bit 양자화 방식: `nf4` 선택 LLM에 최적화된 새로운 4-bit 방식\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # 연산 시 bfloat16 데이터 타입 사용하여 연산 안정성 증가 ( float16보다 안정적 )\n",
    "    max_seq_length=512,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map={\"\":0},\n",
    "    # torch_dtype=torch.bfloat16, #양자화 안할 거면 사용\n",
    "    quantization_config=bnb_config, #양자화 할거면 사용\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "tokenizer.pad_token = \"<|finetune_right_pad_id|>\"\n",
    "tokenizer.pad_token_id = 128004\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "샘플이 너무 적으면 과적합(Overfitting) 위험이 크므로 데이터를 증강(Augmentation)하는 것이 필요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 40 examples [00:00, 1399.05 examples/s]\n",
      "Map: 100%|██████████| 40/40 [00:00<00:00, 5308.24 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# LLaMA 3 Chat 템플릿 적용\n",
    "chat_template = \"\"\"<|begin_of_text|><|start_header_id|>지시사항<|end_header_id|>\n",
    "{SYSTEM}<|eot_id|><|start_header_id|>입력<|end_header_id|>\n",
    "{INPUT}<|eot_id|><|start_header_id|>응답<|end_header_id|>\n",
    "{OUTPUT}<|eot_id|>\"\"\"\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "    \n",
    "    for instruction, output in zip(instructions, outputs):\n",
    "        text = chat_template.format(\n",
    "            SYSTEM=\"아래는 작업을 설명하는 지시사항입니다. 입력된 내용을 바탕으로 적절한 응답을 작성하세요.\",\n",
    "            INPUT= instruction,\n",
    "            OUTPUT= output \n",
    "        ) \n",
    "        \n",
    "        texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "# 데이터셋 로드\n",
    "dataset = load_dataset(\"json\", data_files=\"../00_Data/KoAlpaca_train.json\")\n",
    "# 데이터셋 변환\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True, remove_columns=['instruction', 'output'])\n",
    "\n",
    "split_data = dataset['train'].train_test_split(test_size=0.05, seed=42)\n",
    "train_data, val_data = split_data['train'], split_data['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 url  \\\n",
      "0  https://kin.naver.com/qna/detail.naver?d1id=8&...   \n",
      "1  https://kin.naver.com/qna/detail.naver?d1id=11...   \n",
      "2  https://kin.naver.com/qna/detail.naver?d1id=11...   \n",
      "3  https://kin.naver.com/qna/detail.naver?d1id=7&...   \n",
      "4  https://kin.naver.com/qna/detail.naver?d1id=6&...   \n",
      "\n",
      "                                                text  \n",
      "0  <|begin_of_text|><|start_header_id|>지시사항<|end_...  \n",
      "1  <|begin_of_text|><|start_header_id|>지시사항<|end_...  \n",
      "2  <|begin_of_text|><|start_header_id|>지시사항<|end_...  \n",
      "3  <|begin_of_text|><|start_header_id|>지시사항<|end_...  \n",
      "4  <|begin_of_text|><|start_header_id|>지시사항<|end_...  \n"
     ]
    }
   ],
   "source": [
    "df = train_data.to_pandas()  # \"train\" 데이터셋을 pandas로 변환\n",
    "print(df.head())  # 첫 5개 샘플 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()#훈련 시 메모리 절약 (출력값을 필요할 때만 계산)\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LoRA(PEFT) 설정을 적용하여 기존 모델을 효율적으로 미세 조정\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",# LoRA를 적용할 작업 유형 (CAUSAL_LM: 언어 모델)\n",
    "    r=8,# LoRA 랭크 (적은 수록 가벼움, 크면 성능 향상 가능)\n",
    "    lora_alpha=16,   # 일반적으로 LoRA의 효과를 조절하는 파라미터 (값이 크면 LoRA 가중치의 영향 증가)\n",
    "    lora_dropout=0.05, # Dropout 확률 (일반적으로 0~0.1 추천)\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama-3.1-8B-Instruct-2-4-2-0.001\n",
      "./01_Models/01_RoLaModels/Llama-3.1-8B-Instruct-2-4-2-0.001\n"
     ]
    }
   ],
   "source": [
    "#하이퍼 파라미터 설정\n",
    "\n",
    "epoch = 2 # 전체 데이터셋을 몇 번 반복해서 학습할 것인지\n",
    "batch_size =4\n",
    "gradient_step =2\n",
    "learningrate = 1e-3 # 1e-3 ~ 1e-6 가 일반적인 러닝 레이트 범위 ( 1e-4 에서 시작하는거 추천 )\n",
    "# 1e-3 (0.001) → 매우 높은 학습률, 빠른 학습 가능하지만 불안정할 수도 있음\n",
    "# 5e-4 (0.0005) → 비교적 빠른 학습, 안정성도 고려한 값\n",
    "# 1e-4 (0.0001) → 일반적으로 많이 사용되는 기본값\n",
    "# 5e-5 (0.00005) → 안정성과 학습 속도 균형이 적절한 값\n",
    "# 1e-5 (0.00001) → 비교적 낮은 학습률, 정밀한 파인튜닝에 적합\n",
    "# 5e-6 (0.000005) → 매우 낮은 학습률, 기존 모델을 크게 변경하지 않으면서 미세 조정할 때 유용\n",
    "# 1e-6 (0.000001) → 극도로 낮은 학습률, 작은 변화만 필요할 때 사용\n",
    "step = 300 # 최대 학습 스텝\n",
    "\n",
    "outName = f\"{model_id.split('/')[-1]}-{epoch}-{batch_size}-{gradient_step}-{learningrate}\"\n",
    "output_dir = f\"../01_Models/01_RoLaModels/{outName}\"\n",
    "\n",
    "print(outName)\n",
    "print(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wanDB 사용할거면 실행\n",
    "import wandb\n",
    "wandb.login(key=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DKSYSTEMS\\miniconda3\\envs\\dseek\\lib\\site-packages\\transformers\\training_args.py:1609: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\DKSYSTEMS\\AppData\\Local\\Temp\\ipykernel_26560\\228844736.py:44: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n",
      "Converting train dataset to ChatML: 100%|██████████| 38/38 [00:00<00:00, 4209.26 examples/s]\n",
      "Applying chat template to train dataset: 100%|██████████| 38/38 [00:00<00:00, 5159.71 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 38/38 [00:00<00:00, 947.22 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 38/38 [00:00<00:00, 2637.53 examples/s]\n",
      "Converting eval dataset to ChatML: 100%|██████████| 2/2 [00:00<00:00, 340.97 examples/s]\n",
      "Applying chat template to eval dataset: 100%|██████████| 2/2 [00:00<00:00, 316.99 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 2/2 [00:00<00:00, 230.49 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 2/2 [00:00<00:00, 271.25 examples/s]\n",
      "Using auto half precision backend\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "Currently training with a batch size of: 4\n",
      "The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: url, text. If url, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 38\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 10\n",
      "  Number of trainable parameters = 20,971,520\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msangin1-park\u001b[0m (\u001b[33msangin1-park-dongkuk-systems-brasil\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\DKSYSTEMS\\D\\0.Project\\MY\\AI\\Llama3.1-finetuning\\02_Scripts\\wandb\\run-20250307_160224-8eu0zchk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sangin1-park-dongkuk-systems-brasil/huggingface/runs/8eu0zchk' target=\"_blank\">./01_Models/01_RoLaModels/Llama-3.1-8B-Instruct-2-4-2-0.001</a></strong> to <a href='https://wandb.ai/sangin1-park-dongkuk-systems-brasil/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sangin1-park-dongkuk-systems-brasil/huggingface' target=\"_blank\">https://wandb.ai/sangin1-park-dongkuk-systems-brasil/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sangin1-park-dongkuk-systems-brasil/huggingface/runs/8eu0zchk' target=\"_blank\">https://wandb.ai/sangin1-park-dongkuk-systems-brasil/huggingface/runs/8eu0zchk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:46, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.326800</td>\n",
       "      <td>1.671333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.494800</td>\n",
       "      <td>1.456326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: url, text. If url, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./01_Models/01_RoLaModels/Llama-3.1-8B-Instruct-2-4-2-0.001\\checkpoint-5\n",
      "loading configuration file config.json from cache at C:\\Users\\DKSYSTEMS\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.1-8B-Instruct\\snapshots\\0e9e39f249a16976918f6564b8830bc894c89659\\config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.50.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./01_Models/01_RoLaModels/Llama-3.1-8B-Instruct-2-4-2-0.001\\checkpoint-5\\tokenizer_config.json\n",
      "Special tokens file saved in ./01_Models/01_RoLaModels/Llama-3.1-8B-Instruct-2-4-2-0.001\\checkpoint-5\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: url, text. If url, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./01_Models/01_RoLaModels/Llama-3.1-8B-Instruct-2-4-2-0.001\\checkpoint-10\n",
      "loading configuration file config.json from cache at C:\\Users\\DKSYSTEMS\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.1-8B-Instruct\\snapshots\\0e9e39f249a16976918f6564b8830bc894c89659\\config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.50.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./01_Models/01_RoLaModels/Llama-3.1-8B-Instruct-2-4-2-0.001\\checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./01_Models/01_RoLaModels/Llama-3.1-8B-Instruct-2-4-2-0.001\\checkpoint-10\\special_tokens_map.json\n",
      "Saving model checkpoint to ./01_Models/01_RoLaModels/Llama-3.1-8B-Instruct-2-4-2-0.001\\checkpoint-10\n",
      "loading configuration file config.json from cache at C:\\Users\\DKSYSTEMS\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.1-8B-Instruct\\snapshots\\0e9e39f249a16976918f6564b8830bc894c89659\\config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.50.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./01_Models/01_RoLaModels/Llama-3.1-8B-Instruct-2-4-2-0.001\\checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./01_Models/01_RoLaModels/Llama-3.1-8B-Instruct-2-4-2-0.001\\checkpoint-10\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=2.0066962242126465, metrics={'train_runtime': 53.6433, 'train_samples_per_second': 1.417, 'train_steps_per_second': 0.186, 'total_flos': 1340934079905792.0, 'train_loss': 2.0066962242126465})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "import wandb\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    # max_seq_length=512,\n",
    "    # dataset_text_field=\"text\",\n",
    "    # packing=False,\n",
    "\n",
    "    per_device_train_batch_size=batch_size, # 배치 크기 (GPU당 샘플 개수)\n",
    "    gradient_accumulation_steps=gradient_step,  # 메모리 최적화 Gradient Accumulation 누적 스텝 (메모리 부족 시 증가 가능)\n",
    "    gradient_checkpointing=True, # 활성화하면 GPU 메모리 사용 감소 가능\n",
    "\n",
    "    num_train_epochs=epoch, \n",
    "    # max_steps=step ,  \n",
    "\n",
    "    optim=\"adamw_torch\", # paged_adamw_8bit (VRAM절약 성능 하락) adamw_torch(정확도 높음 메모리사용량 높음)\n",
    "\n",
    "    learning_rate=learningrate,  # 학습률 (기본 2e-4)\n",
    "    lr_scheduler_type=\"linear\", # 학습률 스케줄러 종류 ( linear, cosine, constant )\n",
    "\n",
    "    fp16=True, #정밀도\n",
    "    bf16=False, # 이거 사용하면 오류남 왜인진 모르겠음;; 아마  (A100 GPU같이 고성능 gpu에서 쓰이면 좋아서 그런듯 후...)\n",
    "   \n",
    "    weight_decay=0.01, # 모델이 과적합(Overfitting)되는 것을 방지하기 위해 가중치(Weight)에 패널티를 부여하는 기법  ( 일반화 성능 향상 )\n",
    "\n",
    "    warmup_ratio=0.1, # 상승곡선\n",
    "\n",
    "    seed=42,\n",
    "    \n",
    "    evaluation_strategy=\"steps\", # eval_steps마다 평가\n",
    "    eval_steps=5, # eval 훈련 스텝이 xx번 진행될 때마다 검증 데이터셋 평가\n",
    "\n",
    "    logging_steps=2,\n",
    "    output_dir= output_dir,\n",
    "    save_strategy=\"epoch\",\n",
    "    log_level=\"debug\",\n",
    "\n",
    "    report_to=\"wandb\", # none, wandb 사용  \n",
    ")\n",
    "\n",
    "# Trainer Setup\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    peft_config=lora_config,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=train_args,\n",
    ")\n",
    "\n",
    "# wandb 사용할거면 주석풀고 사용하세요. \n",
    "# wandb.finish() # 이전 실행 종료 (안 하면 새로운 실행이 안 생길 수도 있음)\n",
    "\n",
    "# # wandb 사용\n",
    "# wandb.init(\n",
    "#     project=\"exaone-learning_rate\",\n",
    "#     name=outName,\n",
    "#     reinit=True,\n",
    "#     config={\n",
    "#         \"learning_rate\": learningrate,\n",
    "#         \"batch_size\": batch_size,\n",
    "#         \"gradient_accumulation_steps\": gradient_step,\n",
    "#         \"num_train_epochs\": epoch\n",
    "#     }\n",
    "# )\n",
    "\n",
    "model.config.use_cache = False\n",
    "# 학습 시작\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # 모델의 가중치는 변경하지 않고, forward 연산만 수행함.\n",
    "model.config.use_cache = True  # 이전 계산 결과를 저장하고 사용\t추론 속도 빨라짐, 메모리 사용 증가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RoLA모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./01_Models/01_RoLaModels/Llama-3.1-8B-Instruct-2-4-2-0.001/LoRA\n",
      "loading configuration file config.json from cache at C:\\Users\\DKSYSTEMS\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.1-8B-Instruct\\snapshots\\0e9e39f249a16976918f6564b8830bc894c89659\\config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.50.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./01_Models/01_RoLaModels/Llama-3.1-8B-Instruct-2-4-2-0.001/LoRA\\tokenizer_config.json\n",
      "Special tokens file saved in ./01_Models/01_RoLaModels/Llama-3.1-8B-Instruct-2-4-2-0.001/LoRA\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at ./01_Models/01_RoLaModels/Llama-3.1-8B-Instruct-2-4-2-0.001/LoRA\n"
     ]
    }
   ],
   "source": [
    "# Save Model\n",
    "saveLoRA_dir = f\"{output_dir}/LoRA\"\n",
    "\n",
    "trainer.save_model(saveLoRA_dir)\n",
    "print(f\"Model saved at {saveLoRA_dir}\")\n",
    "\n",
    "# Save Model HugginFace\n",
    "# model.save_pretrained(save_dir)\n",
    "# tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM 응답: 지시사항\n",
      "아래는 작업을 설명하는 지시사항입니다. 입력된 내용을 바탕으로 적절한 응답을 작성하세요.\n",
      "입력\n",
      "다시 합창 합시다' 처럼 거꾸로 읽어도 같은 문장이 영어에도 있나요? 또한 다른 나라의 언어에도 있는 건가요?\n",
      "응답\n",
      "'다시 합창 합시다' 처럼 거꾸로 읽어도 같은 문장이 영어에도 있어요. 'Madam, I'm Adam'이 대표적인 예시입니다. 또한, 다른 나라의 언어에도 이러한 문장이 있습니다. '아래는 몇 가지 예시입니다.\n"
     ]
    }
   ],
   "source": [
    "# 채팅 스타일 프롬프트 (Llama-3의 Chat 모델용)\n",
    "chat_prompt = \"\"\"<|begin_of_text|><|start_header_id|>지시사항<|end_header_id|>\n",
    "아래는 작업을 설명하는 지시사항입니다. 입력된 내용을 바탕으로 적절한 응답을 작성하세요.<|eot_id|>\n",
    "<|start_header_id|>입력<|end_header_id|>\n",
    "다시 합창 합시다' 처럼 거꾸로 읽어도 같은 문장이 영어에도 있나요? 또한 다른 나라의 언어에도 있는 건가요?<|eot_id|>\n",
    "<|start_header_id|>응답<|end_header_id|>\n",
    "\"\"\"\n",
    "# 토큰화 및 모델 실행\n",
    "input_ids = tokenizer(chat_prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(input_ids, max_new_tokens=100, temperature=0.7, top_p=0.9, do_sample=True)\n",
    "\n",
    "# 출력 변환\n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(\"LLM 응답:\", output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  기존 모델과 로라 병합 (어뎁터 유지) 풀파인튜닝 저장 \n",
    "\n",
    "merge_and_unload 양자화 상태로 해버리면 로라 어뎁터가 망가져버림 그래서 양자화 하지 않고 저장해줘야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\DKSYSTEMS\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.1-8B-Instruct\\snapshots\\0e9e39f249a16976918f6564b8830bc894c89659\\config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.50.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\DKSYSTEMS\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.1-8B-Instruct\\snapshots\\0e9e39f249a16976918f6564b8830bc894c89659\\model.safetensors.index.json\n",
      "Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ]\n",
      "}\n",
      "\n",
      "c:\\Users\\DKSYSTEMS\\miniconda3\\envs\\dseek\\lib\\site-packages\\accelerate\\utils\\modeling.py:1536: UserWarning: Current model requires 128 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.38s/it]\n",
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-3.1-8B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\DKSYSTEMS\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.1-8B-Instruct\\snapshots\\0e9e39f249a16976918f6564b8830bc894c89659\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "c:\\Users\\DKSYSTEMS\\miniconda3\\envs\\dseek\\lib\\site-packages\\accelerate\\utils\\modeling.py:1536: UserWarning: Current model requires 256 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n",
      "loading file tokenizer.json from cache at C:\\Users\\DKSYSTEMS\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.1-8B-Instruct\\snapshots\\0e9e39f249a16976918f6564b8830bc894c89659\\tokenizer.json\n",
      "loading file tokenizer.model from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\DKSYSTEMS\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.1-8B-Instruct\\snapshots\\0e9e39f249a16976918f6564b8830bc894c89659\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\DKSYSTEMS\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.1-8B-Instruct\\snapshots\\0e9e39f249a16976918f6564b8830bc894c89659\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "#로라 모델경로를 확인해봐야 해요\n",
    "peft_model_id = saveLoRA_dir\n",
    "\n",
    "loadModel = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,  # float16로 유지 bfloat16\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "loadModel = PeftModel.from_pretrained(loadModel, peft_model_id, device_map=\"auto\")\n",
    "loadtokenizer = AutoTokenizer.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../01_Models/02_FullFinetuningModels\\config.json\n",
      "Configuration saved in ../01_Models/02_FullFinetuningModels\\generation_config.json\n",
      "The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at ../01_Models/02_FullFinetuningModels\\model.safetensors.index.json.\n",
      "tokenizer config file saved in ../01_Models/02_FullFinetuningModels\\tokenizer_config.json\n",
      "Special tokens file saved in ../01_Models/02_FullFinetuningModels\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../01_Models/02_FullFinetuningModels\\\\tokenizer_config.json',\n",
       " '../01_Models/02_FullFinetuningModels\\\\special_tokens_map.json',\n",
       " '../01_Models/02_FullFinetuningModels\\\\tokenizer.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadModel = loadModel.merge_and_unload() #실제 병합\n",
    "merged_model_path = \"../01_Models/02_FullFinetuningModels\"\n",
    "loadModel.save_pretrained(merged_model_path)\n",
    "loadtokenizer.save_pretrained(merged_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GGUF  llama cpp 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참조\n",
    "\n",
    "https://m.blog.naver.com/112fkdldjs/223513042256\n",
    "\n",
    "https://github.com/ollama/ollama/issues/4442\n",
    "\n",
    "https://github.com/ollama/ollama/issues/4572\n",
    "\n",
    "https://github.com/teddylee777/langserve_ollama/tree/main/ollama-modelfile/Llama-3-8B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8_0: 8비트 양자화 모델로, 원본 모델의 품질을 거의 그대로 유지하면서 크기를 절반으로 줄입니다.\n",
    "\n",
    "Q6_K, Q5_K_M, Q5_K_S: 6비트와 5비트 양자화 모델들로, 품질 손실은 미미하지만 크기가 더 작습니다.\n",
    "\n",
    "Q4_K_M, Q4_K_S: 4비트 양자화 모델로, 약간의 품질 손실이 있지만 크기가 매우 작습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../llama.cpp/convert_hf_to_gguf.py ../01_Models/02_FullFinetuningModels --outtype q8_0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
