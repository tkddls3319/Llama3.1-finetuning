{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### í™˜ê²½ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q -U bitsandbytes\n",
    "# !pip install -q -U git+https://github.com/huggingface/transformers.git \n",
    "# !pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "# !pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "# !pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì°¸ì¡°\n",
    "\n",
    "https://github.com/Beomi/KoAlpaca\n",
    "\n",
    "https://colab.research.google.com/gist/Beomi/f163a6c04a869d18ee1a025b6d33e6d8/2023_05_26_bnb_4bit_koalpaca_v1_1a_on_polyglot_ko_12_8b.ipynb\n",
    "\n",
    "https://wikidocs.net/238524\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DKSYSTEMS\\miniconda3\\envs\\dseek\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to find CUDA.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CUDA ì‚¬ìš© ê°€ëŠ¥:\", torch.cuda.is_available())\n",
    "print(\"GPU ì´ë¦„:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "print(\"Torch CUDA ì§€ì› ì—¬ë¶€:\", torch.cuda.is_available())\n",
    "print(\"CUDA ë²„ì „:\", torch.version.cuda)\n",
    "print(\"PyTorch ë²„ì „:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ëª¨ë¸, í† í¬ë‚˜ì´ì € ë¡œë“œ ë° LoRA ì„¤ì •, DataLoad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í•´ë‹¹ ëª¨ë¸ì€ metaì— í‚¤ë“±ë¡ì„ í•´ì•¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:05<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     device_map=\"auto\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "# )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=False)\n",
    "\n",
    "# tokenizer.pad_token = \"<|finetune_right_pad_id|>\"\n",
    "# tokenizer.pad_token_id = 128004\n",
    "# tokenizer.padding_side = 'right'\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # 4-bit ì–‘ìí™” í™œì„±í™”\n",
    "    bnb_4bit_use_double_quant=True, # ì´ì¤‘ ì–‘ìí™”(Double Quantization) ì ìš©í•˜ì—¬ ë” ì ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©\n",
    "    bnb_4bit_quant_type=\"nf4\",    # 4-bit ì–‘ìí™” ë°©ì‹: `nf4` ì„ íƒ LLMì— ìµœì í™”ëœ ìƒˆë¡œìš´ 4-bit ë°©ì‹\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # ì—°ì‚° ì‹œ bfloat16 ë°ì´í„° íƒ€ì… ì‚¬ìš©í•˜ì—¬ ì—°ì‚° ì•ˆì •ì„± ì¦ê°€ ( float16ë³´ë‹¤ ì•ˆì •ì  )\n",
    "    max_seq_length=512,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map={\"\":0},\n",
    "    # torch_dtype=torch.bfloat16, #ì–‘ìí™” ì•ˆí•  ê±°ë©´ ì‚¬ìš©\n",
    "    quantization_config=bnb_config, #ì–‘ìí™” í• ê±°ë©´ ì‚¬ìš©\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "tokenizer.pad_token = \"<|finetune_right_pad_id|>\"\n",
    "tokenizer.pad_token_id = 128004\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìƒ˜í”Œì´ ë„ˆë¬´ ì ìœ¼ë©´ ê³¼ì í•©(Overfitting) ìœ„í—˜ì´ í¬ë¯€ë¡œ ë°ì´í„°ë¥¼ ì¦ê°•(Augmentation)í•˜ëŠ” ê²ƒì´ í•„ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 40 examples [00:00, 1399.05 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:00<00:00, 5308.24 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# LLaMA 3 Chat í…œí”Œë¦¿ ì ìš©\n",
    "chat_template = \"\"\"<|begin_of_text|><|start_header_id|>ì§€ì‹œì‚¬í•­<|end_header_id|>\n",
    "{SYSTEM}<|eot_id|><|start_header_id|>ì…ë ¥<|end_header_id|>\n",
    "{INPUT}<|eot_id|><|start_header_id|>ì‘ë‹µ<|end_header_id|>\n",
    "{OUTPUT}<|eot_id|>\"\"\"\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "    \n",
    "    for instruction, output in zip(instructions, outputs):\n",
    "        text = chat_template.format(\n",
    "            SYSTEM=\"ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ì§€ì‹œì‚¬í•­ì…ë‹ˆë‹¤. ì…ë ¥ëœ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì ì ˆí•œ ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\",\n",
    "            INPUT= instruction,\n",
    "            OUTPUT= output \n",
    "        ) \n",
    "        \n",
    "        texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "dataset = load_dataset(\"json\", data_files=\"../00_Data/KoAlpaca_train.json\")\n",
    "# ë°ì´í„°ì…‹ ë³€í™˜\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True, remove_columns=['instruction', 'output'])\n",
    "\n",
    "split_data = dataset['train'].train_test_split(test_size=0.05, seed=42)\n",
    "train_data, val_data = split_data['train'], split_data['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 url  \\\n",
      "0  https://kin.naver.com/qna/detail.naver?d1id=8&...   \n",
      "1  https://kin.naver.com/qna/detail.naver?d1id=11...   \n",
      "2  https://kin.naver.com/qna/detail.naver?d1id=11...   \n",
      "3  https://kin.naver.com/qna/detail.naver?d1id=7&...   \n",
      "4  https://kin.naver.com/qna/detail.naver?d1id=6&...   \n",
      "\n",
      "                                                text  \n",
      "0  <|begin_of_text|><|start_header_id|>ì§€ì‹œì‚¬í•­<|end_...  \n",
      "1  <|begin_of_text|><|start_header_id|>ì§€ì‹œì‚¬í•­<|end_...  \n",
      "2  <|begin_of_text|><|start_header_id|>ì§€ì‹œì‚¬í•­<|end_...  \n",
      "3  <|begin_of_text|><|start_header_id|>ì§€ì‹œì‚¬í•­<|end_...  \n",
      "4  <|begin_of_text|><|start_header_id|>ì§€ì‹œì‚¬í•­<|end_...  \n"
     ]
    }
   ],
   "source": [
    "df = train_data.to_pandas()  # \"train\" ë°ì´í„°ì…‹ì„ pandasë¡œ ë³€í™˜\n",
    "print(df.head())  # ì²« 5ê°œ ìƒ˜í”Œ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()#í›ˆë ¨ ì‹œ ë©”ëª¨ë¦¬ ì ˆì•½ (ì¶œë ¥ê°’ì„ í•„ìš”í•  ë•Œë§Œ ê³„ì‚°)\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LoRA(PEFT) ì„¤ì •ì„ ì ìš©í•˜ì—¬ ê¸°ì¡´ ëª¨ë¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ ë¯¸ì„¸ ì¡°ì •\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",# LoRAë¥¼ ì ìš©í•  ì‘ì—… ìœ í˜• (CAUSAL_LM: ì–¸ì–´ ëª¨ë¸)\n",
    "    r=8,# LoRA ë­í¬ (ì ì€ ìˆ˜ë¡ ê°€ë²¼ì›€, í¬ë©´ ì„±ëŠ¥ í–¥ìƒ ê°€ëŠ¥)\n",
    "    lora_alpha=16,   # ì¼ë°˜ì ìœ¼ë¡œ LoRAì˜ íš¨ê³¼ë¥¼ ì¡°ì ˆí•˜ëŠ” íŒŒë¼ë¯¸í„° (ê°’ì´ í¬ë©´ LoRA ê°€ì¤‘ì¹˜ì˜ ì˜í–¥ ì¦ê°€)\n",
    "    lora_dropout=0.05, # Dropout í™•ë¥  (ì¼ë°˜ì ìœ¼ë¡œ 0~0.1 ì¶”ì²œ)\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama-3.1-8B-Instruct-2-4-2-0.001\n",
      "./01_Models/01_RoLaModels/Llama-3.1-8B-Instruct-2-4-2-0.001\n"
     ]
    }
   ],
   "source": [
    "#í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "\n",
    "epoch = 2 # ì „ì²´ ë°ì´í„°ì…‹ì„ ëª‡ ë²ˆ ë°˜ë³µí•´ì„œ í•™ìŠµí•  ê²ƒì¸ì§€\n",
    "batch_size =4\n",
    "gradient_step =2\n",
    "learningrate = 1e-3 # 1e-3 ~ 1e-6 ê°€ ì¼ë°˜ì ì¸ ëŸ¬ë‹ ë ˆì´íŠ¸ ë²”ìœ„ ( 1e-4 ì—ì„œ ì‹œì‘í•˜ëŠ”ê±° ì¶”ì²œ )\n",
    "# 1e-3 (0.001) â†’ ë§¤ìš° ë†’ì€ í•™ìŠµë¥ , ë¹ ë¥¸ í•™ìŠµ ê°€ëŠ¥í•˜ì§€ë§Œ ë¶ˆì•ˆì •í•  ìˆ˜ë„ ìˆìŒ\n",
    "# 5e-4 (0.0005) â†’ ë¹„êµì  ë¹ ë¥¸ í•™ìŠµ, ì•ˆì •ì„±ë„ ê³ ë ¤í•œ ê°’\n",
    "# 1e-4 (0.0001) â†’ ì¼ë°˜ì ìœ¼ë¡œ ë§ì´ ì‚¬ìš©ë˜ëŠ” ê¸°ë³¸ê°’\n",
    "# 5e-5 (0.00005) â†’ ì•ˆì •ì„±ê³¼ í•™ìŠµ ì†ë„ ê· í˜•ì´ ì ì ˆí•œ ê°’\n",
    "# 1e-5 (0.00001) â†’ ë¹„êµì  ë‚®ì€ í•™ìŠµë¥ , ì •ë°€í•œ íŒŒì¸íŠœë‹ì— ì í•©\n",
    "# 5e-6 (0.000005) â†’ ë§¤ìš° ë‚®ì€ í•™ìŠµë¥ , ê¸°ì¡´ ëª¨ë¸ì„ í¬ê²Œ ë³€ê²½í•˜ì§€ ì•Šìœ¼ë©´ì„œ ë¯¸ì„¸ ì¡°ì •í•  ë•Œ ìœ ìš©\n",
    "# 1e-6 (0.000001) â†’ ê·¹ë„ë¡œ ë‚®ì€ í•™ìŠµë¥ , ì‘ì€ ë³€í™”ë§Œ í•„ìš”í•  ë•Œ ì‚¬ìš©\n",
    "step = 300 # ìµœëŒ€ í•™ìŠµ ìŠ¤í…\n",
    "\n",
    "outName = f\"{model_id.split('/')[-1]}-{epoch}-{batch_size}-{gradient_step}-{learningrate}\"\n",
    "output_dir = f\"../01_Models/01_RoLaModels/{outName}\"\n",
    "\n",
    "print(outName)\n",
    "print(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wanDB ì‚¬ìš©í• ê±°ë©´ ì‹¤í–‰\n",
    "import wandb\n",
    "wandb.login(key=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DKSYSTEMS\\miniconda3\\envs\\dseek\\lib\\site-packages\\transformers\\training_args.py:1609: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\DKSYSTEMS\\AppData\\Local\\Temp\\ipykernel_26560\\228844736.py:44: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n",
      "Converting train dataset to ChatML: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:00<00:00, 4209.26 examples/s]\n",
      "Applying chat template to train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:00<00:00, 5159.71 examples/s]\n",
      "Tokenizing train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:00<00:00, 947.22 examples/s]\n",
      "Truncating train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:00<00:00, 2637.53 examples/s]\n",
      "Converting eval dataset to ChatML: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 340.97 examples/s]\n",
      "Applying chat template to eval dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 316.99 examples/s]\n",
      "Tokenizing eval dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 230.49 examples/s]\n",
      "Truncating eval dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 271.25 examples/s]\n",
      "Using auto half precision backend\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "Currently training with a batch size of: 4\n",
      "The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: url, text. If url, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 38\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 10\n",
      "  Number of trainable parameters = 20,971,520\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msangin1-park\u001b[0m (\u001b[33msangin1-park-dongkuk-systems-brasil\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\DKSYSTEMS\\D\\0.Project\\MY\\AI\\Llama3.1-finetuning\\02_Scripts\\wandb\\run-20250307_160224-8eu0zchk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sangin1-park-dongkuk-systems-brasil/huggingface/runs/8eu0zchk' target=\"_blank\">./01_Models/01_RoLaModels/Llama-3.1-8B-Instruct-2-4-2-0.001</a></strong> to <a href='https://wandb.ai/sangin1-park-dongkuk-systems-brasil/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sangin1-park-dongkuk-systems-brasil/huggingface' target=\"_blank\">https://wandb.ai/sangin1-park-dongkuk-systems-brasil/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sangin1-park-dongkuk-systems-brasil/huggingface/runs/8eu0zchk' target=\"_blank\">https://wandb.ai/sangin1-park-dongkuk-systems-brasil/huggingface/runs/8eu0zchk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:46, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.326800</td>\n",
       "      <td>1.671333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.494800</td>\n",
       "      <td>1.456326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: url, text. If url, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./01_Models/01_RoLaModels/Llama-3.1-8B-Instruct-2-4-2-0.001\\checkpoint-5\n",
      "loading configuration file config.json from cache at C:\\Users\\DKSYSTEMS\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.1-8B-Instruct\\snapshots\\0e9e39f249a16976918f6564b8830bc894c89659\\config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.50.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./01_Models/01_RoLaModels/Llama-3.1-8B-Instruct-2-4-2-0.001\\checkpoint-5\\tokenizer_config.json\n",
      "Special tokens file saved in ./01_Models/01_RoLaModels/Llama-3.1-8B-Instruct-2-4-2-0.001\\checkpoint-5\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: url, text. If url, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./01_Models/01_RoLaModels/Llama-3.1-8B-Instruct-2-4-2-0.001\\checkpoint-10\n",
      "loading configuration file config.json from cache at C:\\Users\\DKSYSTEMS\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.1-8B-Instruct\\snapshots\\0e9e39f249a16976918f6564b8830bc894c89659\\config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.50.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./01_Models/01_RoLaModels/Llama-3.1-8B-Instruct-2-4-2-0.001\\checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./01_Models/01_RoLaModels/Llama-3.1-8B-Instruct-2-4-2-0.001\\checkpoint-10\\special_tokens_map.json\n",
      "Saving model checkpoint to ./01_Models/01_RoLaModels/Llama-3.1-8B-Instruct-2-4-2-0.001\\checkpoint-10\n",
      "loading configuration file config.json from cache at C:\\Users\\DKSYSTEMS\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.1-8B-Instruct\\snapshots\\0e9e39f249a16976918f6564b8830bc894c89659\\config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.50.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./01_Models/01_RoLaModels/Llama-3.1-8B-Instruct-2-4-2-0.001\\checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./01_Models/01_RoLaModels/Llama-3.1-8B-Instruct-2-4-2-0.001\\checkpoint-10\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=2.0066962242126465, metrics={'train_runtime': 53.6433, 'train_samples_per_second': 1.417, 'train_steps_per_second': 0.186, 'total_flos': 1340934079905792.0, 'train_loss': 2.0066962242126465})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "import wandb\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    # max_seq_length=512,\n",
    "    # dataset_text_field=\"text\",\n",
    "    # packing=False,\n",
    "\n",
    "    per_device_train_batch_size=batch_size, # ë°°ì¹˜ í¬ê¸° (GPUë‹¹ ìƒ˜í”Œ ê°œìˆ˜)\n",
    "    gradient_accumulation_steps=gradient_step,  # ë©”ëª¨ë¦¬ ìµœì í™” Gradient Accumulation ëˆ„ì  ìŠ¤í… (ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ì¦ê°€ ê°€ëŠ¥)\n",
    "    gradient_checkpointing=True, # í™œì„±í™”í•˜ë©´ GPU ë©”ëª¨ë¦¬ ì‚¬ìš© ê°ì†Œ ê°€ëŠ¥\n",
    "\n",
    "    num_train_epochs=epoch, \n",
    "    # max_steps=step ,  \n",
    "\n",
    "    optim=\"adamw_torch\", # paged_adamw_8bit (VRAMì ˆì•½ ì„±ëŠ¥ í•˜ë½) adamw_torch(ì •í™•ë„ ë†’ìŒ ë©”ëª¨ë¦¬ì‚¬ìš©ëŸ‰ ë†’ìŒ)\n",
    "\n",
    "    learning_rate=learningrate,  # í•™ìŠµë¥  (ê¸°ë³¸ 2e-4)\n",
    "    lr_scheduler_type=\"linear\", # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì¢…ë¥˜ ( linear, cosine, constant )\n",
    "\n",
    "    fp16=True, #ì •ë°€ë„\n",
    "    bf16=False, # ì´ê±° ì‚¬ìš©í•˜ë©´ ì˜¤ë¥˜ë‚¨ ì™œì¸ì§„ ëª¨ë¥´ê² ìŒ;; ì•„ë§ˆ  (A100 GPUê°™ì´ ê³ ì„±ëŠ¥ gpuì—ì„œ ì“°ì´ë©´ ì¢‹ì•„ì„œ ê·¸ëŸ°ë“¯ í›„...)\n",
    "   \n",
    "    weight_decay=0.01, # ëª¨ë¸ì´ ê³¼ì í•©(Overfitting)ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ê°€ì¤‘ì¹˜(Weight)ì— íŒ¨ë„í‹°ë¥¼ ë¶€ì—¬í•˜ëŠ” ê¸°ë²•  ( ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ )\n",
    "\n",
    "    warmup_ratio=0.1, # ìƒìŠ¹ê³¡ì„ \n",
    "\n",
    "    seed=42,\n",
    "    \n",
    "    evaluation_strategy=\"steps\", # eval_stepsë§ˆë‹¤ í‰ê°€\n",
    "    eval_steps=5, # eval í›ˆë ¨ ìŠ¤í…ì´ xxë²ˆ ì§„í–‰ë  ë•Œë§ˆë‹¤ ê²€ì¦ ë°ì´í„°ì…‹ í‰ê°€\n",
    "\n",
    "    logging_steps=2,\n",
    "    output_dir= output_dir,\n",
    "    save_strategy=\"epoch\",\n",
    "    log_level=\"debug\",\n",
    "\n",
    "    report_to=\"wandb\", # none, wandb ì‚¬ìš©  \n",
    ")\n",
    "\n",
    "# Trainer Setup\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    peft_config=lora_config,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=train_args,\n",
    ")\n",
    "\n",
    "# wandb ì‚¬ìš©í• ê±°ë©´ ì£¼ì„í’€ê³  ì‚¬ìš©í•˜ì„¸ìš”. \n",
    "# wandb.finish() # ì´ì „ ì‹¤í–‰ ì¢…ë£Œ (ì•ˆ í•˜ë©´ ìƒˆë¡œìš´ ì‹¤í–‰ì´ ì•ˆ ìƒê¸¸ ìˆ˜ë„ ìˆìŒ)\n",
    "\n",
    "# # wandb ì‚¬ìš©\n",
    "# wandb.init(\n",
    "#     project=\"exaone-learning_rate\",\n",
    "#     name=outName,\n",
    "#     reinit=True,\n",
    "#     config={\n",
    "#         \"learning_rate\": learningrate,\n",
    "#         \"batch_size\": batch_size,\n",
    "#         \"gradient_accumulation_steps\": gradient_step,\n",
    "#         \"num_train_epochs\": epoch\n",
    "#     }\n",
    "# )\n",
    "\n",
    "model.config.use_cache = False\n",
    "# í•™ìŠµ ì‹œì‘\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ëŠ” ë³€ê²½í•˜ì§€ ì•Šê³ , forward ì—°ì‚°ë§Œ ìˆ˜í–‰í•¨.\n",
    "model.config.use_cache = True  # ì´ì „ ê³„ì‚° ê²°ê³¼ë¥¼ ì €ì¥í•˜ê³  ì‚¬ìš©\tì¶”ë¡  ì†ë„ ë¹¨ë¼ì§, ë©”ëª¨ë¦¬ ì‚¬ìš© ì¦ê°€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RoLAëª¨ë¸ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./01_Models/01_RoLaModels/Llama-3.1-8B-Instruct-2-4-2-0.001/LoRA\n",
      "loading configuration file config.json from cache at C:\\Users\\DKSYSTEMS\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.1-8B-Instruct\\snapshots\\0e9e39f249a16976918f6564b8830bc894c89659\\config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.50.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./01_Models/01_RoLaModels/Llama-3.1-8B-Instruct-2-4-2-0.001/LoRA\\tokenizer_config.json\n",
      "Special tokens file saved in ./01_Models/01_RoLaModels/Llama-3.1-8B-Instruct-2-4-2-0.001/LoRA\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at ./01_Models/01_RoLaModels/Llama-3.1-8B-Instruct-2-4-2-0.001/LoRA\n"
     ]
    }
   ],
   "source": [
    "# Save Model\n",
    "saveLoRA_dir = f\"{output_dir}/LoRA\"\n",
    "\n",
    "trainer.save_model(saveLoRA_dir)\n",
    "print(f\"Model saved at {saveLoRA_dir}\")\n",
    "\n",
    "# Save Model HugginFace\n",
    "# model.save_pretrained(save_dir)\n",
    "# tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM ì‘ë‹µ: ì§€ì‹œì‚¬í•­\n",
      "ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ì§€ì‹œì‚¬í•­ì…ë‹ˆë‹¤. ì…ë ¥ëœ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì ì ˆí•œ ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\n",
      "ì…ë ¥\n",
      "ë‹¤ì‹œ í•©ì°½ í•©ì‹œë‹¤' ì²˜ëŸ¼ ê±°ê¾¸ë¡œ ì½ì–´ë„ ê°™ì€ ë¬¸ì¥ì´ ì˜ì–´ì—ë„ ìˆë‚˜ìš”? ë˜í•œ ë‹¤ë¥¸ ë‚˜ë¼ì˜ ì–¸ì–´ì—ë„ ìˆëŠ” ê±´ê°€ìš”?\n",
      "ì‘ë‹µ\n",
      "'ë‹¤ì‹œ í•©ì°½ í•©ì‹œë‹¤' ì²˜ëŸ¼ ê±°ê¾¸ë¡œ ì½ì–´ë„ ê°™ì€ ë¬¸ì¥ì´ ì˜ì–´ì—ë„ ìˆì–´ìš”. 'Madam, I'm Adam'ì´ ëŒ€í‘œì ì¸ ì˜ˆì‹œì…ë‹ˆë‹¤. ë˜í•œ, ë‹¤ë¥¸ ë‚˜ë¼ì˜ ì–¸ì–´ì—ë„ ì´ëŸ¬í•œ ë¬¸ì¥ì´ ìˆìŠµë‹ˆë‹¤. 'ì•„ë˜ëŠ” ëª‡ ê°€ì§€ ì˜ˆì‹œì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# ì±„íŒ… ìŠ¤íƒ€ì¼ í”„ë¡¬í”„íŠ¸ (Llama-3ì˜ Chat ëª¨ë¸ìš©)\n",
    "chat_prompt = \"\"\"<|begin_of_text|><|start_header_id|>ì§€ì‹œì‚¬í•­<|end_header_id|>\n",
    "ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ì§€ì‹œì‚¬í•­ì…ë‹ˆë‹¤. ì…ë ¥ëœ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì ì ˆí•œ ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.<|eot_id|>\n",
    "<|start_header_id|>ì…ë ¥<|end_header_id|>\n",
    "ë‹¤ì‹œ í•©ì°½ í•©ì‹œë‹¤' ì²˜ëŸ¼ ê±°ê¾¸ë¡œ ì½ì–´ë„ ê°™ì€ ë¬¸ì¥ì´ ì˜ì–´ì—ë„ ìˆë‚˜ìš”? ë˜í•œ ë‹¤ë¥¸ ë‚˜ë¼ì˜ ì–¸ì–´ì—ë„ ìˆëŠ” ê±´ê°€ìš”?<|eot_id|>\n",
    "<|start_header_id|>ì‘ë‹µ<|end_header_id|>\n",
    "\"\"\"\n",
    "# í† í°í™” ë° ëª¨ë¸ ì‹¤í–‰\n",
    "input_ids = tokenizer(chat_prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(input_ids, max_new_tokens=100, temperature=0.7, top_p=0.9, do_sample=True)\n",
    "\n",
    "# ì¶œë ¥ ë³€í™˜\n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(\"LLM ì‘ë‹µ:\", output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ê¸°ì¡´ ëª¨ë¸ê³¼ ë¡œë¼ ë³‘í•© (ì–´ëí„° ìœ ì§€) í’€íŒŒì¸íŠœë‹ ì €ì¥ \n",
    "\n",
    "merge_and_unload ì–‘ìí™” ìƒíƒœë¡œ í•´ë²„ë¦¬ë©´ ë¡œë¼ ì–´ëí„°ê°€ ë§ê°€ì ¸ë²„ë¦¼ ê·¸ë˜ì„œ ì–‘ìí™” í•˜ì§€ ì•Šê³  ì €ì¥í•´ì¤˜ì•¼í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\DKSYSTEMS\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.1-8B-Instruct\\snapshots\\0e9e39f249a16976918f6564b8830bc894c89659\\config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.50.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\DKSYSTEMS\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.1-8B-Instruct\\snapshots\\0e9e39f249a16976918f6564b8830bc894c89659\\model.safetensors.index.json\n",
      "Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ]\n",
      "}\n",
      "\n",
      "c:\\Users\\DKSYSTEMS\\miniconda3\\envs\\dseek\\lib\\site-packages\\accelerate\\utils\\modeling.py:1536: UserWarning: Current model requires 128 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:05<00:00,  1.38s/it]\n",
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-3.1-8B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\DKSYSTEMS\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.1-8B-Instruct\\snapshots\\0e9e39f249a16976918f6564b8830bc894c89659\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "c:\\Users\\DKSYSTEMS\\miniconda3\\envs\\dseek\\lib\\site-packages\\accelerate\\utils\\modeling.py:1536: UserWarning: Current model requires 256 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n",
      "loading file tokenizer.json from cache at C:\\Users\\DKSYSTEMS\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.1-8B-Instruct\\snapshots\\0e9e39f249a16976918f6564b8830bc894c89659\\tokenizer.json\n",
      "loading file tokenizer.model from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\DKSYSTEMS\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.1-8B-Instruct\\snapshots\\0e9e39f249a16976918f6564b8830bc894c89659\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\DKSYSTEMS\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.1-8B-Instruct\\snapshots\\0e9e39f249a16976918f6564b8830bc894c89659\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "#ë¡œë¼ ëª¨ë¸ê²½ë¡œë¥¼ í™•ì¸í•´ë´ì•¼ í•´ìš”\n",
    "peft_model_id = saveLoRA_dir\n",
    "\n",
    "loadModel = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,  # float16ë¡œ ìœ ì§€ bfloat16\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "loadModel = PeftModel.from_pretrained(loadModel, peft_model_id, device_map=\"auto\")\n",
    "loadtokenizer = AutoTokenizer.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../01_Models/02_FullFinetuningModels\\config.json\n",
      "Configuration saved in ../01_Models/02_FullFinetuningModels\\generation_config.json\n",
      "The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at ../01_Models/02_FullFinetuningModels\\model.safetensors.index.json.\n",
      "tokenizer config file saved in ../01_Models/02_FullFinetuningModels\\tokenizer_config.json\n",
      "Special tokens file saved in ../01_Models/02_FullFinetuningModels\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../01_Models/02_FullFinetuningModels\\\\tokenizer_config.json',\n",
       " '../01_Models/02_FullFinetuningModels\\\\special_tokens_map.json',\n",
       " '../01_Models/02_FullFinetuningModels\\\\tokenizer.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadModel = loadModel.merge_and_unload() #ì‹¤ì œ ë³‘í•©\n",
    "merged_model_path = \"../01_Models/02_FullFinetuningModels\"\n",
    "loadModel.save_pretrained(merged_model_path)\n",
    "loadtokenizer.save_pretrained(merged_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GGUF  llama cpp ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì°¸ì¡°\n",
    "\n",
    "https://m.blog.naver.com/112fkdldjs/223513042256\n",
    "\n",
    "https://github.com/ollama/ollama/issues/4442\n",
    "\n",
    "https://github.com/ollama/ollama/issues/4572\n",
    "\n",
    "https://github.com/teddylee777/langserve_ollama/tree/main/ollama-modelfile/Llama-3-8B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8_0: 8ë¹„íŠ¸ ì–‘ìí™” ëª¨ë¸ë¡œ, ì›ë³¸ ëª¨ë¸ì˜ í’ˆì§ˆì„ ê±°ì˜ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë©´ì„œ í¬ê¸°ë¥¼ ì ˆë°˜ìœ¼ë¡œ ì¤„ì…ë‹ˆë‹¤.\n",
    "\n",
    "Q6_K, Q5_K_M, Q5_K_S: 6ë¹„íŠ¸ì™€ 5ë¹„íŠ¸ ì–‘ìí™” ëª¨ë¸ë“¤ë¡œ, í’ˆì§ˆ ì†ì‹¤ì€ ë¯¸ë¯¸í•˜ì§€ë§Œ í¬ê¸°ê°€ ë” ì‘ìŠµë‹ˆë‹¤.\n",
    "\n",
    "Q4_K_M, Q4_K_S: 4ë¹„íŠ¸ ì–‘ìí™” ëª¨ë¸ë¡œ, ì•½ê°„ì˜ í’ˆì§ˆ ì†ì‹¤ì´ ìˆì§€ë§Œ í¬ê¸°ê°€ ë§¤ìš° ì‘ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../llama.cpp/convert_hf_to_gguf.py ../01_Models/02_FullFinetuningModels --outtype q8_0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
