{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"your model Name\")\n",
    "\n",
    "default_system_prompt = (\n",
    "    \"당신은 사용자의 요청에만 충실하게 답변하는 사실 기반의 중립적인 AI 어시스턴트입니다.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=default_system_prompt),\n",
    "    HumanMessage(content=\"질문 쓰는 곳\")\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader\n",
    "import os\n",
    "\n",
    "llm = ChatOllama(model=\"your model Name\")\n",
    "\n",
    "def load_and_split_documents(loaders):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "    all_splits = []\n",
    "    \n",
    "    for loader in loaders:\n",
    "        docs = loader.load_and_split()\n",
    "        splits = text_splitter.split_documents(docs)\n",
    "        all_splits.extend(splits)\n",
    "\n",
    "    return all_splits\n",
    "\n",
    "def get_loaders_from_directory(folder_path):\n",
    "    loaders = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        full_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            loaders.append(PyPDFLoader(full_path))\n",
    "        elif filename.endswith(\".txt\"):\n",
    "            loaders.append(TextLoader(full_path, encoding=\"utf-8\"))\n",
    "        # elif filename.endswith(\".docx\"):\n",
    "        #     loaders.append(Docx2txtLoader(full_path))\n",
    "        # elif filename.endswith(\".md\"):\n",
    "        #     loaders.append(MarkdownLoader(full_path))  # 필요시 확장 가능\n",
    "\n",
    "    return loaders\n",
    "\n",
    "#모든 경로에 파일 다읽어오게게\n",
    "loaders = get_loaders_from_directory('your File Path')\n",
    "\n",
    "documents = load_and_split_documents(loaders)\n",
    "\n",
    "model_name = \"jhgan/ko-sroberta-multitask\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents = documents , embedding = embedding_model)\n",
    "\n",
    "custom_template = \"\"\"당신은 사용자의 요청에만 충실하게 답변하는 사실 기반의 중립적인 AI 어시스턴트입니다.\n",
    "\n",
    "[문서 정보]\n",
    "{context}\n",
    "\n",
    "[사용자 질문]\n",
    "{question}\n",
    "\n",
    "[응답]\n",
    "\"\"\"\n",
    "QA_PROMPT = PromptTemplate(template=custom_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    output_key=\"answer\" \n",
    ")\n",
    "\n",
    "# 체인 구성\n",
    "chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 5}),\n",
    "    memory=memory,\n",
    "    combine_docs_chain_kwargs={\"prompt\": QA_PROMPT},\n",
    "    return_source_documents=True,\n",
    "    output_key=\"answer\"  \n",
    ")\n",
    "\n",
    "def AskLLM(ask : str):\n",
    "    response = chain.invoke({\"question\": f\"{ask}\"})\n",
    "    print_response_summary(response=response, memory=memory, max_docs=1, max_history=4)\n",
    "    return response\n",
    "\n",
    "def print_response_summary(response, memory=None, max_docs=1, max_history=4):\n",
    "\n",
    "    print(\" [AI 응답] :\", response.get('answer', '[응답 없음]'))\n",
    "\n",
    "    source_docs = response.get(\"source_documents\", [])[:max_docs]\n",
    "    if source_docs:\n",
    "        doc = source_docs[0]\n",
    "        print(\"\\n[문서 1번]\")\n",
    "        print(\"내용 (요약) :\", doc.page_content[:50])\n",
    "        print(\"-[출처 메타데이터] : \", doc.metadata)\n",
    "    else:\n",
    "        print(\"\\n 참고 문서 없음\")\n",
    "\n",
    "    if memory:\n",
    "        print(\"\\n [최근 대화 히스토리]\")\n",
    "        # 최근 N개의 메시지만 출력\n",
    "        messages = memory.chat_memory.messages[-max_history:]\n",
    "        for msg in messages:\n",
    "            role = \"    [사용자]\" if msg.type == \"human\" else \"    [AI]\"\n",
    "            print(f\"{role}: {msg.content}\")\n",
    "                             \n",
    "# # 응답 출력\n",
    "# print(\"응답:\", response['answer'])\n",
    "\n",
    "# # 참고 문서 출력\n",
    "# source_docs = response.get(\"source_documents\", [])\n",
    "# for i, doc in enumerate(source_docs):\n",
    "#     print(f\"\\n문서 {i+1}:\")\n",
    "#     print(\"내용 (요약):\", doc.page_content[:300])\n",
    "#     print(\"출처 메타데이터:\", doc.metadata)\n",
    "\n",
    "# for msg in memory.chat_memory.messages:\n",
    "#     role = \" 사용자\" if msg.type == \"human\" else \" AI\"\n",
    "#     print(f\"{role}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = AskLLM(\"이 문서의 핵심은?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dseek",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
