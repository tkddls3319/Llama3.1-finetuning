{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "# !pip install unsloth\n",
    "\n",
    "# !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "# # We have to check which Torch version for Xformers (2.3 -> 0.0.27)\n",
    "# from torch import __version__; from packaging.version import Version as V\n",
    "# xformers = \"xformers==0.0.27\" if V(__version__) < V(\"2.4.0\") else \"xformers\"\n",
    "# !pip install --no-deps {xformers} trl peft accelerate bitsandbytes triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n",
    "print(\"Current Device:\", torch.cuda.current_device() if torch.cuda.is_available() else \"None\")\n",
    "print(\"CUDA 사용 가능:\", torch.cuda.is_available())\n",
    "\n",
    "print(\"Torch CUDA 지원 여부:\", torch.cuda.is_available())\n",
    "print(\"CUDA 버전:\", torch.version.cuda)\n",
    "print(\"PyTorch 버전:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login(key=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "max_seq_length = 1024 # Choose any! We auto support RoPE Scaling internally!\n",
    "model_id = \"unsloth/Llama-3.1-8B-Instruct\"\n",
    "# model_id = \"unsloth/gemma-3-12b-it\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_id, # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = None,# None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "    load_in_4bit = True, # False for LoRA 16bit\n",
    "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils  import format_dataset, ModelType\n",
    "\n",
    "# EXAONE, gemma, LLaMA3, Alpaca\n",
    "train_dataset, test_dataset = format_dataset(\n",
    "    ModelType.LLAMA,\n",
    "    train_data_files=\"../00_Data/KoAlpaca_train.json\",\n",
    ")\n",
    "\n",
    "# 실행 결과 확인\n",
    "# print(\"Train Dataset Example:\")\n",
    "# print(train_dataset[\"train\"][0][\"text\"])\n",
    "\n",
    "# if test_dataset:\n",
    "#     print(\"\\nTest Dataset Example:\")\n",
    "#     print(test_dataset[\"train\"][0][\"text\"])\n",
    "\n",
    "from unsloth import standardize_sharegpt\n",
    "train_dataset = standardize_sharegpt(train_dataset['train'])\n",
    "# test_dataset = standardize_sharegpt(test_dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_r = 8  # 8, 16, 32, 64, 128 중 선택 가능. 클수록 성능 향상, 작을수록 메모리 절약.\n",
    "lora_alpha = 16 # 16 일반적으로 LoRA의 효과를 조절하는 파라미터 (값이 크면 LoRA 가중치의 영향 증가)\n",
    "lora_dropout = 0.05\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_r, \n",
    "    lora_alpha = lora_alpha,  \n",
    "    lora_dropout = lora_dropout,  # 0이면 최적화된 성능을 보장. 작은 값으로 설정하면 과적합 방지 가능.\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],  \n",
    "    bias = \"none\",  # \"none\"이 최적화된 값. 필요에 따라 \"all\" 또는 \"lora_only\"로 변경 가능.\n",
    "\n",
    "    # Unsloth의 최적화된 VRAM 사용 방식 적용 (최대 30% 절약, 2배 큰 배치 크기 가능)\n",
    "    use_gradient_checkpointing = \"unsloth\",  # \"unsloth\" 또는 True 사용 가능. 메모리 절약 효과.\n",
    "    random_state = 3407,  # 실험의 일관성을 유지하기 위한 랜덤 시드 값.\n",
    "    # Rank Stabilized LoRA (RSLoRA) 사용 여부\n",
    "    use_rslora = False,  # False면 일반 LoRA 사용. True면 RSLoRA 적용 가능 (추가적인 안정성 제공).\n",
    "    loftq_config = None,  # None이면 사용 안 함. 사용하면 LoftQ 기반 저비트 양자화 적용 가능.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하이퍼 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 2 # 전체 데이터셋을 몇 번 반복해서 학습할 것인지\n",
    "batch_size = 4\n",
    "gradient_step = 2 \n",
    "learningrate = 2e-4 # 1e-3 ~ 1e-6 가 일반적인 러닝 레이트 범위 ( 1e-4 에서 시작하는거 추천 )\n",
    "#7e-4\n",
    "# 1e-3 (0.001) → 매우 높은 학습률, 빠른 학습 가능하지만 불안정할 수도 있음\n",
    "# 5e-4 (0.0005) → 비교적 빠른 학습, 안정성도 고려한 값\n",
    "# 1e-4 (0.0001) → 일반적으로 많이 사용되는 기본값\n",
    "# 5e-5 (0.00005) → 안정성과 학습 속도 균형이 적절한 값\n",
    "# 1e-5 (0.00001) → 비교적 낮은 학습률, 정밀한 파인튜닝에 적합\n",
    "# 5e-6 (0.000005) → 매우 낮은 학습률, 기존 모델을 크게 변경하지 않으면서 미세 조정할 때 유용\n",
    "# 1e-6 (0.000001) → 극도로 낮은 학습률, 작은 변화만 필요할 때 사용\n",
    "step = 300 # 최대 학습 스텝\n",
    "\n",
    "wandb_project = f\"finetuning\"\n",
    "\n",
    "outName = f\"unsloth-{model_id.split('/')[-1]}-{epoch}-{batch_size}-{gradient_step}-{learningrate}-{lora_r}-{lora_alpha}-{lora_dropout}\"\n",
    "output_dir = f\"../99_GitLoss/01_RoLaModels/{outName}\"\n",
    "\n",
    "print(outName)\n",
    "print(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,  # 학습할 모델 (LoRA 적용된 LLaMA 등)\n",
    "    tokenizer = tokenizer,  # 토크나이저 (문장을 토큰 단위로 변환)\n",
    "\n",
    "    train_dataset = train_dataset,  # 학습에 사용할 데이터셋\n",
    "    # eval_dataset=test_dataset, # 검증 데이터\n",
    "\n",
    "    dataset_text_field = \"text\",  # 데이터셋에서 텍스트 필드 이름 지정\n",
    "\n",
    "    max_seq_length = max_seq_length,  # 최대 시퀀스 길이 설정\n",
    "    dataset_num_proc = 1,  # 데이터 전처리 멀티프로세싱 개수 설정 (속도 최적화)\n",
    "\n",
    "    packing = False,  # ✅ `True`로 설정하면 짧은 시퀀스를 묶어 학습 속도를 5배 증가 가능!\n",
    "\n",
    "    # 학습 파라미터 설정\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = batch_size,  # GPU 당 배치 크기 (메모리에 따라 조절 가능)\n",
    "        gradient_accumulation_steps = gradient_step,  # 그래디언트 누적 스텝 (메모리 절약용)\n",
    "\n",
    "        num_train_epochs=epoch, \n",
    "        # max_steps = 60,  # 최대 학습 스텝 수\n",
    "\n",
    "        optim = \"adamw_torch\",  # ✅ 8bit Adam 옵티마이저 사용 (메모리 절약)\n",
    "\n",
    "        learning_rate = learningrate,  # 학습률 설정\n",
    "        lr_scheduler_type = \"linear\",  # 선형 학습률 스케줄 적용 (학습률이 선형적으로 감소)\n",
    "\n",
    "        fp16 = not is_bfloat16_supported(),  # GPU 환경에 따라 FP16 사용 여부 자동 선택\n",
    "        bf16 = is_bfloat16_supported(),  # Ampere 이상 GPU에서는 BF16 사용  (A100, RTX 30/40)은 BF16 사용\n",
    "    \n",
    "        weight_decay = 0.01,  # 가중치 감소 (Regularization)\n",
    "\n",
    "        warmup_ratio=0.1, # 상승곡선\n",
    "        # warmup_steps = 5,  # 초기 워밍업 스텝 설정\n",
    "\n",
    "        seed = 3407,  # 랜덤 시드 설정 (재현성 보장)\n",
    "\n",
    "        evaluation_strategy=\"steps\", # eval_steps마다 평가\n",
    "        eval_steps=5, # eval 훈련 스텝이 xx번 진행될 때마다 검증 데이터셋 평가\n",
    "\n",
    "        logging_steps = 2,  # 학습 로그 출력 간격\n",
    "        output_dir = output_dir,  # 학습 결과 저장 경로\n",
    "\n",
    "        save_strategy=\"epoch\",\n",
    "        log_level=\"debug\",\n",
    "\n",
    "        report_to = \"wandb\",  # \"wandb\" 또는 \"tensorboard\"로 변경 가능\n",
    "    ),\n",
    ")\n",
    "\n",
    "# 이전 실행 종료 (안 하면 새로운 실행이 안 생길 수도 있음)\n",
    "wandb.finish()\n",
    "\n",
    "# wandb 사용\n",
    "wandb.init(\n",
    "    project= wandb_project,\n",
    "    name=outName,\n",
    "    reinit=True,\n",
    "    config={\n",
    "        \"learning_rate\": learningrate,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"gradient_accumulation_steps\": gradient_step,\n",
    "        \"num_train_epochs\": epoch\n",
    "    }\n",
    ")\n",
    "\n",
    "trainer_stats = trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "질의 응답 2가지 버전 ( 완성결과 한번에 출력, 실시간으로 출력 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a helpful AI assistant.<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "지구에 대해 알려줘?<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    chat_prompt\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 100, use_cache = True)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"다시 합창 합시다' 처럼 거꾸로 읽어도 같은 문장이 영어에도 있나요? 또한 다른 나라의 언어에도 있는 건가요?.\", # instruction\n",
    "        \"\", # input\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chat_prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a helpful AI assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "다시 합창 합시다' 처럼 거꾸로 읽어도 같은 문장이 영어에도 있나요? 또한 다른 나라의 언어에도 있는 건가요?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "   chat_prompt\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델을 저장하고 불러오는 방법 ( Unsloth방식, Huggingface 방식 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA 어댑터만 저장\n",
    "saveLoRA_dir = f\"{output_dir}/LoRA\"\n",
    "model.save_pretrained(saveLoRA_dir) # Local saving\n",
    "tokenizer.save_pretrained(saveLoRA_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로라 모델 호출 자동으로 병합되는듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "loadmodel, loadtokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = saveLoRA_dir, # YOUR MODEL YOU USED FOR TRAINING\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = None,# None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "    load_in_4bit = True, # False for LoRA 16bit\n",
    ")\n",
    "FastLanguageModel.for_inference(loadmodel) # Enable native 2x faster inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chat_prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a helpful AI assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "다시 합창 합시다' 처럼 거꾸로 읽어도 같은 문장이 영어에도 있나요? 또한 다른 나라의 언어에도 있는 건가요?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "inputs = loadtokenizer(\n",
    "[\n",
    "   chat_prompt\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(loadtokenizer)\n",
    "_ = loadmodel.generate(**inputs, streamer = text_streamer, max_new_tokens = 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LoRA 모델을 float16 또는 4bit로 병합하여 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge to 16bit\n",
    "if True: loadmodel.save_pretrained_merged(f\"{output_dir}/modle\", loadtokenizer, save_method = \"merged_16bit\",)\n",
    "\n",
    "# Merge to 4bit\n",
    "if False: model.save_pretrained_merged(f\"{output_dir}/modle\", tokenizer, save_method = \"merged_4bit\",)\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False: model.save_pretrained_merged(f\"{output_dir}/modle\", tokenizer, save_method = \"lora\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GGUF / llama.cpp 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save to 8bit Q8_0\n",
    "# if False: model.save_pretrained_gguf(f\"{output_dir}/modle\", tokenizer)\n",
    "\n",
    "# # Save to 16bit GGUF\n",
    "# if True: model.save_pretrained_gguf(f\"{output_dir}/modle\", tokenizer, quantization_method = \"f16\")\n",
    "\n",
    "# # Save to q4_k_m GGUF\n",
    "# if False: model.save_pretrained_gguf(f\"{output_dir}/gguf\", tokenizer, quantization_method = \"q4_k_m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python llama.cpp/convert_hf_to_gguf.py ./your path/modle --outtype q8_0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
